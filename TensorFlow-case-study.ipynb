{
    "cells": [
        {
            "attachments": {
                "ibm-cloud.png": {
                    "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAB1CAMAAADOZ57OAAABUFBMVEX///8AAADExMRiYmLh4eH0+v9GRkZNTU2Dg4NycnKKiooQEBBbW1v29vb8/PzMzMyYmJiqqqqkpKTw8PAmJiYAZ/7Z2dkfHx/p6ek4ODh4eHgtLS0+Pj7IyMigoKC3t7cAY/6rxP6UlJQXFxfa9vetzvqsyvsAa/xtbW2IiIg9k/cAdvm0tLQeoO0Aav3N5v3o+fzj8v3a8/oAXf1d0ekAcfsAmu2J4usUkPIvxeM7jfk6hvs40d802dwgueW73fwAe/g/mvUtvuUAgPVHnfpnyOuz2vyWzfnw9f+Aw/gituZqufd/tvum4/Feqvp70+2UyPvK4f1Sw+mNuvtUpPlek//K2/8AWP+Jr/9cs/IMivTh7P+91v6+5PcVr+ie1/RNjf6Jr/5avO4bqOpcuvBxo/297vWd4fJ83e125OWd6uzD8/Io4teN7eeB4+hL2eEeGSKEAAAM0klEQVR4nO2ca1vTSBuAG9oAAk0rpQVKLRWKoNIjiKAcpELLwYrg4r61Aq64La7s8v+/vTOTzGSSTA5Cguj13B+UTCbTdu7O6cmkoZD/5NfW1vIBlAsEA/j6tQBfvxb5tSvwdRcpdxRRcv7Kztd30PgTqa6sLIrS86s2vv69vPwe7FsCHGjWVmodQXp+dXVV5Ct/eXn5T9BvCrClXKvVVgQ9op2vb8gXdIg/kS4S1rQm2/j6jnT9G/ybAmxRjpCwsiVZ7EteW7u8tGYGbpGT9VrtwJp8sXohWxL/QasyGL1+MtuogVWtyWWrrjLS9e0W3hHgRBU1sG1POb8hXzCZ/+kcIGEnHvLloXndCeT19XUvDew/iCneDZp7e10P2TqrMNm4G1QF3aFoEQ2D1x2k3OleHCwuLl5cXHQ7VVht3WXkand7BbFIQMYunjc7ZWEEH/jplLs42HFEdam+ut1mJw/G7h7ls3U0sT86OkK6DrpNTBfTbHY6YOzOcba3h33VDppVzk252sG6kDAYyH4CJ7s2C+OTvT3sa/tE0I7K+U61ms/nrQEqwvd/YdoYEJWlQuFrRXDi7PQU+doWhBEJcr6KjAkiikjmN1hFBwb2VSjsmutd/oJ1HTnFpOQyxiJMxhF78BUY9QIxVjdUPNZ1enrmMqOQBcI6V0QXRD0Co7K7hIwttUp6kvwV2friHvBVZAQvNf/f6tXV1do3aF1BUmqZOsUv50iXaFCzgHQpurDnq6urV6tXMNkImjppYnXtyLsu1MS45tVZxfw6XeHkMGHS/5LH1JKzY/4XrdFYWlraUP88O0e+POoygHd4PP+FVtHjEqHP/7f8QC1Z6vG9ZEaloTWvyjnyZTeNd6bz3MPAlerhiWipMUNqz4NYVHiZqMAIOWPOz2eYD2dGJWl6vH8ywqf3qpU65L+vWPC+KMpX5MvLvWUVdVb/Iy8wJPEMCFMRo0Mxw2VzJHVeUOAgORMRnCE86OOL7eNK/S18od7wvOEpp1ztdLs4AIyjwJ2q108t9hU2+0LM8d3/PZI0Ym1Gw5KTrzFLwY/Zud/BF+4NW14+QPmk2+0eYLTQ/WLXWyfq3Zc0wQlTfUnD5uKig06+YqPWUofoyd/BV+O85aU3JLZMwlZWLrx0jKlYLJbK4OaD/6JGsK+hbFxjak6tyz79Ms3XaMJU3JTk4Csl+hawUn8DX/J5q/XVNZdS1W6tNDtVTKd5oQpb8bLpA4PHlH4+AfvK8gkx0wfWfElTxoLGqASRr+i00Bct4zfwVUe+Sm6ZyieIZveEb01K9WKF3Ib21ikKfcUNWVLGBoZ9kU7TuKZJ4woftfEl7GQxKXL6N/DVarV23fJUsa6TqiXSW+4SY6JnkSx48KXWNjvCvqYeo3/SfB4iNTUt9jXJ/KTxrDA6yYZOdQj79X21C63WhkueSrUqsoWp4i5xRfBsiwUvviYN3Rz2NRDVGwd32dig0FdU8yGNsiu0uaQUJqOgja8oGWGvIzFCr7stX41Wa98lslHGA5ZdHqXrUZgXXyRIwJZLxFeoH1e2noXUS9TGF600iVty9RNBWoLIV2J+iIQ9BsfD/BI80U9I06KUtPEYEZnKTOPr0pHb87XfarmsveQyal82t5UxTW9d4nV9ETV6JeFipkIRsa+0Vmn864yNSr2sEgW+5ke4US7zgKVHtCQ6JYpqx/dZjji7bHryNnzJshxSWvv7zt2hgnxVHHRpwlwnHZ59GfvDUGhA4iYhPbghJGx8KRmt0gxnBu7rdiy+onOSETYZjWirPLr8s/hK85fFAve1gyzIoRLy5dwdIqtuew9xl7jo1vt78UW+sexI8zWG54L0ez+hXiT2RddeQ+YTDLMvxaxLf0tuvu4brpoO2lepVKrsyKGN/V3LzgADiun+pJADD0OYF1/jkh7+YL5CWfT/uJo0L6kBKrGvHq3O0uYTDLOvuGRFq3EXX1HTVeHBIH3J9fphqb2jhOr7uw1HX/h+smtxSg0Jc4l0ePCFBU1HDYfYVxRrVG9Z4RrBAWCxrwGt7qbMJxgmX3owJBOeoH9OqO/AxdeUJCYQX/JMo75R2kF/1Xd36045FcWDLvUXIVwCHUJfU9GExlhqEmsZ5KZf1BeZ5k/gKsYtrRf/IfZFO7es+QTD5IuurtN4sh8JGy539qUwvVORkBKhq4aAfDXeNWY2iIfGbsNxuqHwN/8dWKnVVpzFCn1NjzDwh53uN8fniS+lT60nEuglFSL2RRfHXn1RJffUk3T1llH4k2JfdPZI/aRMx74ys//unbY7qtFoHDrm9aYrdFKrCX9yRUfoy8hQj+HFmC8ybxxJkD5IXYqJfdHbXl59zWv5aUE0OpJiL2HrizYoNlTS/jEAX6W3b/bfaY2h3mi4Bg+9ICNfwt80Ynjwhb7aw5wx3RdpOfEEq0uffGnvYJwOmTSSPM9ewtZXv3bEuu8A9wO8Rb7oHL4+U/fFV+gACXPsEIW+MnNhypA6IoR1YZwv3N1MDOnfZ3/6wxHDUSiU0Aale+wlbH1pqqfZvZ7g5vOftt6+ZXOMDTTv8KVU/Asejmtm9/spY6RP0RdPnC82ldDqR+zLOGEQYfQ1aPIVpdtx2EvY+aJD3Sj7dgXna2vrjzesJZQ2Dv3xVa6trzsuwTzHN1gAgfeldVX0UOyLRhzi5hMMg6+EZG5fmq9x9hK2vjK35uvwxdaWPoWvHJb88aUcra8LfiFHx4svtRnRA96Xtrai1SP2ldXqrN98gmHwRYcri68J9hJ3wNf7F0+2dtjRTqlU8udm0LYfvsinph4MvhLTXN3Z+KJ1Nmf7Jozty9wf/oCvW+sP5ScvX7zhDkvttpcFsTvbe3uOv9/hyRdZ1tAdbAZfuPFMsNpxjh+OG7ZUJWz3s9mNX+79odJ3W752nr5+PcMdt9vtHdvMP8LB3t6R03nvvmgNGX2hytS3UdvE53tFlTbM7ZIz+hq1aV+G+Yb4fgqdH7JvRlC+Dp+9fs2PWJV25Tobsa1sB+wrNJ/h8onvf9FVEbfJSg3vxzUlRl9aI+mllZ7QBJL9ihFt5w4NRorXX+w9BOVrZvbVU74DlCuVii8DWND9YSjK+bHxxaJCesRXURdlo2rbNPqi8xO6iIrwdT6mrc7oYEhnJ6qvuOEopMdG/Pb1aPbpK4Ofys6OLwPYl9PTm/sid0Ro72XyxWPjS9+ZmtbaTIrewlRLNfqibYJ2edp6YJRkZZOPhP5udENs5wGtS2FX7AOPXr18bfCFdPnhq7x3enrmlMGTL1zfg/TgGr5YNaJ2MT85OaxvpM+y8iV9PwiVqTY+2tzm9PdH1OMDhd1N1uLz9LCXyI2y4JrvvorPZg2+FC83uNypnp6eOu4S9nx/mUUnruGLjStmehX+9ODIyAQORNKArxTO9sSZW21vFb2bJo2kB4b0Td5aD6jvBugb6O/Td6n67etT8VnSOCHED0zevNwz5MvxjiWujsd8gjkelYiQOhhkU67r+KJTcjNa3h49BVtR+gRZacjdtLOb3vC6LzwrjQR0f7mde1b8ZEzyQxcevr7YnYvfS6fTePjOoP/n6KycxHvDerx33PyBr+OLzfGMxKxnSSsS7LbP0NmHSSadUNAZhmnnB+1MfV8vz84W3/tcJqJ6fn5uO3wZn0+hjUy8dVrfLnY9X6FEr6XMEX2/qb5LRk2LmfNyD8gYZIbN+22ihi9Gmmb225fyuTh77P925C9OT2h6f55o4gF32fV8WTfR3OPDHWzI0hxGjB3oEJ+X6z3HE3SGwb5QiQx3WXDxw7+Ss8m234XiZ8hsu8OQsWPpF6Zieo3PUoYl29AtWSnZPl+ZyHJVeS9mPhme5nwhK3qDnEsZ8zKZaYWtl/XH0RS25yYb4P3KysJs8m+/C8VP1PoT5/eLnuF4PJ6dT7nnRF3bPH4CLSuq6shwNh6/b34CTX8V/Pya6IldHzlOFmd9rtuzglPzAm7E4UIxeexriaUC8uVPFBKwcpwr5vycIlbOC4WCY2wDuAntYjKZ/OSez2tx5JeNfCsOsPAoh4R98KmwDfxrb55+XwC4Ln8tJJMLD30pancJUfDnFjVgx99/ImGfb74M28C2lqB1Bc7//swlc7njG3WKcr2wuYl0efttHOBGPCqiPjGXfPn+Q7si/zCV9mGjtbm8vImEuT2tDvjCzvECmnbkcrlnT18hXnJ8/vz5BeWJztamzvLHZcTm8uY7f3brAO58OC5iZclksVicxTzTeKrySuW1zkfGMmFzF1bJt0n74ctibmEhh1HFzXLijNqot4+ateXN1gbYunXkDw8FPBIyo/PpEFwBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwA/zfy+Nv4wqheohAAAAAElFTkSuQmCC"
                }
            },
            "cell_type": "markdown",
            "metadata": {},
            "source": "![ibm-cloud.png](attachment:ibm-cloud.png)\n# CASE STUDY - convolutional neural networks"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport csv\nimport joblib\nimport time\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\nplt.style.use('seaborn')\n%matplotlib inline"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.decomposition import PCA"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Synopsis\n----------\n\nYou were hired at AAVAIL to be a member of a data science team that works closely together.  Some of your first projects\nare meant to help marketing with customer retention and to investigate market specific trends. There are also some\nprojects relating to user comments that are getting off the ground.  However, you will also be working alongside\nthe deep-learning specialists that maintain the core product at AAVAIL---its audio and visual manipulation models.\n\nBecause the team meets regularly all new data science hires are expected to go through a series deep-learning tutorials\nto ensure that they can contribute to conversations about the core product.   The first in this series is the following\ntutorial on CNNs.  You will be guided through the following parts.\n\n  1. Environment setup\n  2. Model scaffolding using Keras\n  3. Logging and Model serialization\n  4. Model iteration"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Fashion MNIST\n\n>One project that the data science team at AAVAIL has been tasked with is ensuring that the video feeds are in fact news video feeds.  There are people that are performing quality assurance on these feeds, but eventually the data science team will need to build a service that samples a number of frames from a video, then identifies objects in the images, flagging for review any feeds that may be different.\n\nA solid benchmark dataset for this task is the Fashion MNIST dataset.  \n\n* training set - 60,000 images\n* test set - 10,000 images\n* images are 28 pixels x 28 pixels\n* classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## PART 1 - environment setup"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "### load the data  \nfashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() \nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', \n               'Sneaker', 'Bag', 'Ankle boot']\nnum_classes = len(class_names)\n\n## Normalize pixel values to be between 0 and 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n\nX_train = train_images\nX_test = test_images\ny_train = train_labels\ny_test = test_labels"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 1\n\nVisualize a sample of the images to QA the data set (for instance plot one image of each class). Then, print a summary of the data (for instance, the shape of training set, the shape of the test set, the number of sample per class...)."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## YOUR CODE HERE (visualization code)\n\ndef show_img(img_ind):\n    fig = plt.figure(figsize=(6,4),dpi=150,facecolor='white')\n    ax = fig.add_subplot(111)\n\n    ax.imshow(train_images[img_ind])\n    ax.grid(False)\n    ax.set_title(class_names[train_labels[img_ind]])\n    \nfig = plt.figure(figsize=(10,8),dpi=150,facecolor='white')\noffset = 25\nfor i in range(25):\n    ax = fig.add_subplot(5,5,i+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.grid(False)\n    ax.imshow(train_images[i+offset], cmap=plt.cm.binary)\n    ax.set_xlabel(class_names[train_labels[i+offset]])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## YOUR CODE HERE (Summarize the data)\n\nprint(\"-------------------------------------------\")\nprint(\"X_train: {}\".format(X_train.shape))\nclass_info = list(sorted(Counter(y_train).items()))\nprint(\"num classes: {}, classes: {}\".format(len(class_info), [i[0] for i in class_info]))\nprint(\"class samples: {}\".format([i[1] for i in class_info]))\nprint(\"class balance: {}\".format([round(i[1]/X_train.shape[0],2) for i in class_info]))\nprint(\"-------------------------------------------\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Take a moment to understand how the data set is built, especially what are the 3 dimensions of X_train and X_test."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 2\n\nIn this question you are asked to build a base model. The base model that we want to build is composed of a PCA model followed by a classic machine learning classifier. The PCA takes as input the images that have been flattened and creates a representation of the images with few features (the first n principal components). Then, the classifier will classify the images based on this reduced representation. Following the best practices we will create a sklearn Pipeline and pass this pipeline in a grid search to optimize the hyper parameters. You are free to use the classifier that you think will perform best in this pipeline."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## YOUR CODE HERE (Replace the symbole #<> with your code)\n\n# First we flatten the images to have a data shape that can be ingested by the PCA model.\n# Take a moment to understand what does this function (flatten()) do to the images \n# and why this step is necessary.\nX_train_flat = np.array([i.flatten() for i in train_images])\nX_test_flat = np.array([i.flatten() for i in test_images])\n\npipe = Pipeline([('pca', PCA(n_components=50)),\n                 ('clf', RandomForestClassifier(n_estimators=100))])\n\n\nif not os.path.isdir(\"saved\"):\n    os.mkdir(\"saved\")\n\n## we create a \"saved\" folder to save the trained model.\nsaved_model = os.path.join(\"saved\",'pca-rf.joblib')\nif not os.path.exists(saved_model):\n    time_start = time.time()\n    pipe.fit(X_train_flat, y_train)\n    print(\"saving the pipeline\")\n    joblib.dump(pipe, saved_model)\n    print(\"train time\", time.strftime('%H:%M:%S', time.gmtime(time.time()-time_start)))\nelse:\n    print(\"loading {} from file\".format(saved_model))\n    pipe = joblib.load(saved_model)\n    "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## YOUR CODE HERE (Replace the symbole #<> with your code)\n# Evaluate your model using the classification_report() function\ny_pred = pipe.predict(X_test_flat)\nprint(\"-->\".join(pipe.named_steps.keys()))\nprint(classification_report(y_test, y_pred, target_names=np.array(class_names)))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## PART 2 -  model scaffolding using Keras\n\nCreate a function that returns a model using ``keras.models.Sequential()`` and ensure that you pass ``activation_function`` as an argument.  Instaintiate a version of the model and print the summary.  This function is just meant to return a simple multilayer perceptron network.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 3\n\nIn this question you are asked to build a function that created a simple multilayer perceptron network. To build a sequential model we first need to initialize the Sequential object, then we sequentially add the layers of the model to that object using the add() method.\n\nIf your are not familiar with the Sequential class of Keras take a quick look at this documentation : https://keras.io/api/models/sequential/\n\nThe following link list all the layers that you can add to a Keras sequential object : https://keras.io/api/layers/\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "## YOUR CODE HERE (Replace the symbole #<> with your code)\n\ndef build_mlp():\n    \"\"\"\n    This Function creates a simple Dense (or multilayer perceptron) network.\n    \"\"\"\n    \n    # Initialize the Sequential object\n    model = keras.Sequential()\n    # add a Flatten layer to the sequence\n    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    # add a Dense layer to the sequence\n    model.add(keras.layers.Dense(128, activation='relu'))\n    # add the last dense layer to the sequence. Because this is the output layer, \n    # the output dimension should be equal to the number of class that your want to predict.\n    model.add(keras.layers.Dense(10, activation='softmax'))   \n\n      \n    return model\n\nmodel_simple = build_mlp()\nmodel_simple.summary()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We encourage you to modify the structure of the network. You can add new layers, change the number of neurons per layer or add dropout layers."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 4\n\nCreate another version of your neural network.  This time you should build a proper CNN.  Remember that one pattern to consider starting from is alternating ``Con2D`` and ``MaxPooling2D`` layers.  This is often followed by a couple of ``Dense`` layers.  Recall that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The output of the last ``Dense`` layer should correspond to the number of classes and generally uses a 'softmax' activation.  Use `model.summary()` to ensure a cohesive architecture."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "## YOUR CODE HERE\n\n\ndef build_cnn(dropout=None):\n    \"\"\"\n    This function creates a convolutional neural network (cnn)\n    \"\"\"\n    \n    if not dropout:\n        dropout = [False,False]\n    \n    model = keras.Sequential()\n    \n    model.add(keras.layers.Conv2D(28, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n    model.add(keras.layers.MaxPooling2D((2, 2)))\n    \n    if dropout[0]:\n        model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(keras.layers.MaxPooling2D((2, 2)))\n    \n    if dropout[0]:\n        model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    \n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64, activation='relu'))\n    model.add(keras.layers.Dense(10, activation='softmax'))\n    return model \n\n\nmodel = build_cnn()\nmodel.summary()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## PART 3 - logging and Model serialization\n\nYou can use a trained model without having to retrain it.  Your can also continue training a model to pick-up training where you left off.  The `tf.keras.callbacks.ModelCheckpoint` callback allows to continually save the model both during and at the end of training.  For long running models this is ideal in case the training is interrupted.  Otherwise you can \nused `model.save()` and `model.load()`.  In this part you will create a function that accomplished a few things at once. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### QUESTION 5\n\nIn this question you will create a function that intend to :\n\n1. save your models so that each iteration only needs to be run once\n2. save the specifics of your model in a log file \n\n  * optimizer \n  * loss_fn \n  * test_loss\n  * test_accuracy\n  * any notes"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## YOUR CODE HERE\n\ndef train_network(model_name, model, loss_fn, X_train, y_train, X_test, y_test, optimizer='adam'):\n    \"\"\"\n    This function compiles, trains and saves the keras model\n    Input : \n        - model_name : the name of the model (we will save the model under this name)\n        - model : The keras Sequential model\n        - loss_fn : the name of the loss function used to train the model (https://keras.io/api/losses/)\n        - opitmizer : the name of the optimizer used to train the model (https://keras.io/api/optimizers/)\n        - X_train : the training data\n        - y_train : the training labels\n        - X_test : the test data\n        - y_test : the test labels\n    \"\"\"\n\n    # First we create a filename for the model joining its name with the \"saved\" directory\n    save_dir = 'saved'\n    if not os.path.isdir(save_dir):\n        os.mkdir(save_dir)\n\n    saved_model = os.path.join(save_dir, \"{}.h5\".format(model_name))\n\n    ## compile the model\n    if not os.path.exists(saved_model):\n        ## compile model using the compile method of the keras.models.Sequential class\n        model.compile(optimizer=optimizer,\n                      loss=loss_fn,\n                      metrics=['accuracy'])\n        ## fit the model using the fit method of the keras.models.Sequential class\n        model.fit(X_train,\n                  y_train,\n                  batch_size=64,\n                  epochs=5,\n                  validation_data=(X_test, y_test))\n        \n        # Save the model\n        model.save(saved_model)\n\n        ## evaluate model\n        test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)    \n        \n        ## save a log file\n        log_file = os.path.join(save_dir,\"{}.log\".format(model_name)) \n        with open(log_file, 'w') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow([\"loss_function\", loss_fn])\n            writer.writerow([\"optimizer\", optimizer])\n            writer.writerow([\"test_loss\", test_loss])\n            writer.writerow([\"test_acc\", test_acc])\n\n    else:\n        print(\"... loading saved model\")\n        model = keras.models.load_model(saved_model)\n                    \n    return model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## PART 4 - model iteration"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Question 6\n\nUsing the functions that you created in the previous questions build the model and train it on the MNIST dataset."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "## YOUR CODE HERE (build and train a mulitlayer perception classifier)\n\nmodel_mlp = build_mlp()\nmodel_mlp = train_network(\"simple_mlp\", model_mlp, \"sparse_categorical_crossentropy\",\n                          X_train,\n                          y_train,\n                          X_test,\n                          y_test,\n                          optimizer='adam')\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## YOUR CODE HERE (build and train a CNN)\n\n## For the CNN we have to add a third dimension to each sample image. \n## This dimension is called channel and is expected by the CNN2D layer. \n## Here the channel value is 1 because we have black and white images. \n## We would have to set this value to 3 for colored images.\nX_train_1 = np.expand_dims(X_train, -1)\nX_test_1 = np.expand_dims(X_test, -1)\n\nmodel_cnn = build_cnn()\nmodel_cnn = train_network(\"cnn\", model_cnn, \"sparse_categorical_crossentropy\", \n                          X_train_1,\n                          y_train,\n                          X_test_1,\n                          y_test,\n                          optimizer='adam')"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}